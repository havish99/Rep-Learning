{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.load(\"reshaped_14.npy\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(w,x,b):\n",
    "    out = np.dot(x,w)+b.T\n",
    "    return out\n",
    "# the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid function\n",
    "def derivative_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    res = x\n",
    "    return res * (res > 0)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    res = x\n",
    "    return res * (res > 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 14)\n",
      "(60000, 196)\n"
     ]
    }
   ],
   "source": [
    "flat = X_train[0].shape\n",
    "print(flat)\n",
    "epochs = 10\n",
    "in_dim = flat[0]*flat[1]\n",
    "X=X_train.reshape(len(X_train),flat[0]*flat[1])\n",
    "print(X.shape)\n",
    "# rescaled\n",
    "X=X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_nodes=40\n",
    "learning_rate = 1e-5\n",
    "Lambda=0.1\n",
    "W_mean = np.random.normal(0,1e-5,(in_dim,hidden_nodes)) # corresponding to mean dimensions\n",
    "W_covar = np.random.normal(0,1e-5,(in_dim,hidden_nodes)) # corresponding to log covariance\n",
    "W_decoder = np.random.normal(0,1e-5,(hidden_nodes,in_dim)) # corresponding to decoder part\n",
    "b_mean=np.random.normal(0,1e-5,(hidden_nodes,1))\n",
    "b_covar=np.random.normal(0,1e-5,(hidden_nodes,1))\n",
    "b_decoder=np.random.normal(0,1e-5,(in_dim,1))\n",
    "nsamples = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(delta,z):\n",
    "    sum1=np.sum(delta,axis=0)\n",
    "    sum1=sum1.reshape(len(sum1),1)\n",
    "    bias = sum1\n",
    "    total = np.matmul(z.T,delta)\n",
    "    return total,bias\n",
    "\n",
    "def smi(delta,W,z):\n",
    "    delta_l1 = np.matmul(delta,W.T)\n",
    "    sm = derivative_sigmoid(z)*delta_l1\n",
    "    return sm\n",
    "\n",
    "def sm_relu(delta,W,z):\n",
    "    delta_l1 = np.matmul(delta,W.T)\n",
    "    sm = derivative_relu(z)*delta_l1\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1621299.285018625\n",
      "epoch: 1 loss: 934159.6174995744\n",
      "epoch: 2 loss: 812914.0430264266\n",
      "epoch: 3 loss: 774986.780861857\n",
      "epoch: 4 loss: 756604.2469461369\n",
      "epoch: 5 loss: 744766.6898205557\n",
      "epoch: 6 loss: 738195.1964611991\n",
      "epoch: 7 loss: 732821.0429646148\n",
      "epoch: 8 loss: 729271.9083741143\n",
      "epoch: 9 loss: 724689.97399136\n",
      "epoch: 10 loss: 722475.8759814985\n",
      "epoch: 11 loss: 719898.5182960989\n",
      "epoch: 12 loss: 717233.2506341678\n",
      "epoch: 13 loss: 715575.9595045334\n",
      "epoch: 14 loss: 713080.2310419815\n",
      "epoch: 15 loss: 711098.2638798507\n",
      "epoch: 16 loss: 709831.9130892553\n",
      "epoch: 17 loss: 707273.9667716295\n",
      "epoch: 18 loss: 705979.2674623342\n",
      "epoch: 19 loss: 704347.0935813136\n",
      "epoch: 20 loss: 702184.1980990529\n",
      "epoch: 21 loss: 701198.5517399225\n",
      "epoch: 22 loss: 699221.7435413995\n",
      "epoch: 23 loss: 698161.7634205192\n",
      "epoch: 24 loss: 696016.9599095992\n",
      "epoch: 25 loss: 694784.672658854\n",
      "epoch: 26 loss: 693545.5078971473\n",
      "epoch: 27 loss: 692229.4507355745\n",
      "epoch: 28 loss: 690889.8986932985\n",
      "epoch: 29 loss: 688843.8567250082\n",
      "epoch: 30 loss: 688083.557996156\n",
      "epoch: 31 loss: 686440.5393342366\n",
      "epoch: 32 loss: 685552.3534578254\n",
      "epoch: 33 loss: 684284.0125780428\n",
      "epoch: 34 loss: 682806.8732787964\n",
      "epoch: 35 loss: 681892.8150774345\n",
      "epoch: 36 loss: 681685.4841849856\n",
      "epoch: 37 loss: 679893.7868955793\n",
      "epoch: 38 loss: 678898.5240473958\n",
      "epoch: 39 loss: 678477.6018715242\n",
      "epoch: 40 loss: 677775.7372201391\n",
      "epoch: 41 loss: 676245.1559881064\n",
      "epoch: 42 loss: 676081.3911780188\n",
      "epoch: 43 loss: 675339.9830523641\n",
      "epoch: 44 loss: 674743.3373391737\n",
      "epoch: 45 loss: 673802.2876330408\n",
      "epoch: 46 loss: 673226.7051806668\n",
      "epoch: 47 loss: 672248.314512118\n",
      "epoch: 48 loss: 672272.4024441411\n",
      "epoch: 49 loss: 671387.1659657405\n",
      "epoch: 50 loss: 670406.4264640042\n",
      "epoch: 51 loss: 669608.5401457611\n",
      "epoch: 52 loss: 669756.3956502443\n",
      "epoch: 53 loss: 669217.9484999238\n",
      "epoch: 54 loss: 667953.2445915259\n",
      "epoch: 55 loss: 667889.0109176156\n",
      "epoch: 56 loss: 667266.7011492918\n",
      "epoch: 57 loss: 666681.4710632\n",
      "epoch: 58 loss: 666784.569436315\n",
      "epoch: 59 loss: 665947.7453953578\n",
      "epoch: 60 loss: 665437.9620417078\n",
      "epoch: 61 loss: 664664.792242724\n",
      "epoch: 62 loss: 664563.6532868625\n",
      "epoch: 63 loss: 664214.1374425732\n",
      "epoch: 64 loss: 663686.5500482632\n",
      "epoch: 65 loss: 663888.6391408603\n",
      "epoch: 66 loss: 662674.482363842\n",
      "epoch: 67 loss: 662291.5172416376\n",
      "epoch: 68 loss: 661612.9211881883\n",
      "epoch: 69 loss: 661551.4769847445\n",
      "epoch: 70 loss: 661811.7981543195\n",
      "epoch: 71 loss: 660816.3344856846\n",
      "epoch: 72 loss: 659976.7669710024\n",
      "epoch: 73 loss: 660311.2031992878\n",
      "epoch: 74 loss: 659590.2644901588\n",
      "epoch: 75 loss: 659969.2611895162\n",
      "epoch: 76 loss: 659505.3179910677\n",
      "epoch: 77 loss: 658590.7111015893\n",
      "epoch: 78 loss: 658413.2121085261\n",
      "epoch: 79 loss: 658315.7273452366\n",
      "epoch: 80 loss: 657852.5337123892\n",
      "epoch: 81 loss: 657144.2265386708\n",
      "epoch: 82 loss: 657682.2832597081\n",
      "epoch: 83 loss: 657826.3261341612\n",
      "epoch: 84 loss: 656993.6845078914\n",
      "epoch: 85 loss: 655825.8642180132\n",
      "epoch: 86 loss: 656170.7395990196\n",
      "epoch: 87 loss: 656087.1646788201\n",
      "epoch: 88 loss: 655623.6684800579\n",
      "epoch: 89 loss: 655352.6580049538\n",
      "epoch: 90 loss: 655369.0226603869\n",
      "epoch: 91 loss: 654106.5904620112\n",
      "epoch: 92 loss: 654649.5535264146\n",
      "epoch: 93 loss: 654213.3397773276\n",
      "epoch: 94 loss: 653901.0530132428\n",
      "epoch: 95 loss: 653140.576685255\n",
      "epoch: 96 loss: 653452.1713489353\n",
      "epoch: 97 loss: 652487.4486936211\n",
      "epoch: 98 loss: 652139.6713799967\n",
      "epoch: 99 loss: 652208.9713365629\n",
      "epoch: 100 loss: 652396.904767642\n",
      "epoch: 101 loss: 651554.5570512897\n",
      "epoch: 102 loss: 651502.5185358223\n",
      "epoch: 103 loss: 651178.6987298586\n",
      "epoch: 104 loss: 650659.6208010445\n",
      "epoch: 105 loss: 650030.0770133013\n",
      "epoch: 106 loss: 649949.2252045718\n",
      "epoch: 107 loss: 650000.0113477799\n",
      "epoch: 108 loss: 649226.6681810351\n",
      "epoch: 109 loss: 648921.8063517972\n",
      "epoch: 110 loss: 649122.3417406328\n",
      "epoch: 111 loss: 648722.8027411184\n",
      "epoch: 112 loss: 648297.5620910162\n",
      "epoch: 113 loss: 647584.5063650646\n",
      "epoch: 114 loss: 647151.984913488\n",
      "epoch: 115 loss: 647793.2321596475\n",
      "epoch: 116 loss: 646851.5178747361\n",
      "epoch: 117 loss: 647015.4227392946\n",
      "epoch: 118 loss: 645658.1138729594\n",
      "epoch: 119 loss: 645385.8679540764\n",
      "epoch: 120 loss: 645481.7781308464\n",
      "epoch: 121 loss: 645139.6803757953\n",
      "epoch: 122 loss: 645176.6686634135\n",
      "epoch: 123 loss: 644967.7138938322\n",
      "epoch: 124 loss: 644755.4564415916\n",
      "epoch: 125 loss: 643490.4702734133\n",
      "epoch: 126 loss: 643523.2576452338\n",
      "epoch: 127 loss: 643136.3104527878\n",
      "epoch: 128 loss: 642600.5438486893\n",
      "epoch: 129 loss: 642485.7437430425\n",
      "epoch: 130 loss: 641604.1803664602\n",
      "epoch: 131 loss: 641314.1328268998\n",
      "epoch: 132 loss: 641346.420818643\n",
      "epoch: 133 loss: 640408.2717729982\n",
      "epoch: 134 loss: 640837.0867183758\n",
      "epoch: 135 loss: 640337.3698835376\n",
      "epoch: 136 loss: 639752.1809012387\n",
      "epoch: 137 loss: 639777.3191300251\n",
      "epoch: 138 loss: 638790.4481631753\n",
      "epoch: 139 loss: 639532.5801013657\n",
      "epoch: 140 loss: 638667.3333212663\n",
      "epoch: 141 loss: 637839.0678696756\n",
      "epoch: 142 loss: 638245.9858283424\n",
      "epoch: 143 loss: 637299.8470819717\n",
      "epoch: 144 loss: 637013.0601024283\n",
      "epoch: 145 loss: 637042.0623510131\n",
      "epoch: 146 loss: 636393.8013681294\n",
      "epoch: 147 loss: 635958.7248071596\n",
      "epoch: 148 loss: 635203.6069379078\n",
      "epoch: 149 loss: 635074.9804222211\n",
      "epoch: 150 loss: 635147.8625744676\n",
      "epoch: 151 loss: 634881.7657700576\n",
      "epoch: 152 loss: 633398.476783779\n",
      "epoch: 153 loss: 633760.9527805403\n",
      "epoch: 154 loss: 633832.051710597\n",
      "epoch: 155 loss: 633327.7160042524\n",
      "epoch: 156 loss: 632944.9593072609\n",
      "epoch: 157 loss: 632609.1021986038\n",
      "epoch: 158 loss: 632146.8238354548\n",
      "epoch: 159 loss: 632195.2492994256\n",
      "epoch: 160 loss: 631792.8483022343\n",
      "epoch: 161 loss: 630693.9508376024\n",
      "epoch: 162 loss: 631649.4008996524\n",
      "epoch: 163 loss: 629777.4240913497\n",
      "epoch: 164 loss: 629894.0494457886\n",
      "epoch: 165 loss: 630188.6232177082\n",
      "epoch: 166 loss: 629762.1743946701\n",
      "epoch: 167 loss: 628997.3598325604\n",
      "epoch: 168 loss: 629037.6611716731\n",
      "epoch: 169 loss: 628482.9053513601\n",
      "epoch: 170 loss: 628019.455639025\n",
      "epoch: 171 loss: 627573.5463748705\n",
      "epoch: 172 loss: 627606.644552562\n",
      "epoch: 173 loss: 627423.3609993756\n",
      "epoch: 174 loss: 626828.059170271\n",
      "epoch: 175 loss: 627264.8198672051\n",
      "epoch: 176 loss: 626750.3068003434\n",
      "epoch: 177 loss: 626389.2360259985\n",
      "epoch: 178 loss: 625694.0955546089\n",
      "epoch: 179 loss: 625469.0076213515\n",
      "epoch: 180 loss: 625349.6647602163\n",
      "epoch: 181 loss: 624932.898864631\n",
      "epoch: 182 loss: 625170.9385480594\n",
      "epoch: 183 loss: 624146.2736654052\n",
      "epoch: 184 loss: 624142.6972978219\n",
      "epoch: 185 loss: 623328.2339614296\n",
      "epoch: 186 loss: 623492.4808926147\n",
      "epoch: 187 loss: 622860.3267235509\n",
      "epoch: 188 loss: 622746.2511951986\n",
      "epoch: 189 loss: 623257.819947093\n",
      "epoch: 190 loss: 622344.1561096453\n",
      "epoch: 191 loss: 622004.6226185242\n",
      "epoch: 192 loss: 621961.9572591659\n",
      "epoch: 193 loss: 622274.8385547358\n",
      "epoch: 194 loss: 621201.539433245\n",
      "epoch: 195 loss: 620614.9113456357\n",
      "epoch: 196 loss: 620971.4389909562\n",
      "epoch: 197 loss: 620630.4596336216\n",
      "epoch: 198 loss: 620408.4196060292\n",
      "epoch: 199 loss: 619888.4642800349\n",
      "epoch: 200 loss: 619128.6737378173\n",
      "epoch: 201 loss: 619626.7052560037\n",
      "epoch: 202 loss: 619691.1080128707\n",
      "epoch: 203 loss: 619163.7701663582\n",
      "epoch: 204 loss: 617901.9718841924\n",
      "epoch: 205 loss: 618699.4402151583\n",
      "epoch: 206 loss: 618351.177209727\n",
      "epoch: 207 loss: 617585.7897715283\n",
      "epoch: 208 loss: 617103.6000489945\n",
      "epoch: 209 loss: 616888.5343217883\n",
      "epoch: 210 loss: 616489.8146751554\n",
      "epoch: 211 loss: 616813.0842859985\n",
      "epoch: 212 loss: 616349.9958714952\n",
      "epoch: 213 loss: 616187.8469588001\n",
      "epoch: 214 loss: 616021.3489530428\n",
      "epoch: 215 loss: 614772.1133627277\n",
      "epoch: 216 loss: 614942.1756855496\n",
      "epoch: 217 loss: 615453.8197975465\n",
      "epoch: 218 loss: 613987.4855834323\n",
      "epoch: 219 loss: 614006.3363077652\n",
      "epoch: 220 loss: 614050.3009259725\n",
      "epoch: 221 loss: 614037.8000204908\n",
      "epoch: 222 loss: 613460.4349873402\n",
      "epoch: 223 loss: 612925.2479829903\n",
      "epoch: 224 loss: 612288.4660326468\n",
      "epoch: 225 loss: 612601.492234078\n",
      "epoch: 226 loss: 611516.0068208077\n",
      "epoch: 227 loss: 611848.0618177197\n",
      "epoch: 228 loss: 611382.7119599422\n",
      "epoch: 229 loss: 611329.2401054397\n",
      "epoch: 230 loss: 611068.051660377\n",
      "epoch: 231 loss: 611117.6886024883\n",
      "epoch: 232 loss: 610466.0915303045\n",
      "epoch: 233 loss: 610282.9847938862\n",
      "epoch: 234 loss: 610180.5003908254\n",
      "epoch: 235 loss: 609964.2677497658\n",
      "epoch: 236 loss: 608586.6340559777\n",
      "epoch: 237 loss: 608729.2682457225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 238 loss: 608844.3348235387\n",
      "epoch: 239 loss: 607932.534534108\n",
      "epoch: 240 loss: 608049.2682277338\n",
      "epoch: 241 loss: 607486.316271285\n",
      "epoch: 242 loss: 607352.5179506629\n",
      "epoch: 243 loss: 607237.6952725867\n",
      "epoch: 244 loss: 606956.9552920867\n",
      "epoch: 245 loss: 606468.513297532\n",
      "epoch: 246 loss: 606530.9385183968\n",
      "epoch: 247 loss: 605652.1339681689\n",
      "epoch: 248 loss: 606167.2805495469\n",
      "epoch: 249 loss: 605264.283202816\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "batchsize=100\n",
    "# learning_rate=1e-5\n",
    "num_iter = (int)(nsamples/batchsize)\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(num_iter):\n",
    "        total_1 = np.zeros(W_mean.shape)\n",
    "        total_2 = np.zeros(W_covar.shape)\n",
    "        total_3 = np.zeros(W_decoder.shape)\n",
    "        bias_1 = np.zeros(b_mean.shape)\n",
    "        bias_2 = np.zeros(b_covar.shape)\n",
    "        bias_3 = np.zeros(b_decoder.shape)\n",
    "    # forward pass\n",
    "        X1 = X[j*batchsize:(j+1)*batchsize]\n",
    "    # the random latent variable\n",
    "        mean = np.zeros(len(b_mean))\n",
    "        var = np.eye(len(b_mean))\n",
    "        a = np.random.multivariate_normal(mean,var,batchsize)\n",
    "#         print(a.shape)\n",
    "        a = a.reshape(len(a),len(a[0]))\n",
    "#     print(a.shape)\n",
    "        out_1 = layer(W_mean,X1,b_mean)\n",
    "        out_2 = layer(W_covar,X1,b_covar)\n",
    "        zmean = sigmoid(out_1)\n",
    "        zlogvar = sigmoid(out_2)\n",
    "    \n",
    "        z = zmean + np.exp(0.5*zlogvar)*a\n",
    "        out_3 = layer(W_decoder,z,b_decoder)\n",
    "        y_pred = sigmoid(out_3)\n",
    "    \n",
    "    # the loss\n",
    "        loss1 = np.sum((y_pred-X1)**2)\n",
    "        loss2 = Lambda*0.5*np.sum(np.exp(zlogvar)+(zmean)**2-1-zlogvar)\n",
    "        loss = loss+loss1+loss2\n",
    "#         print(i,j,loss)\n",
    "    \n",
    "    # backprop\n",
    "    \n",
    "    ## decoder\n",
    "        delta = 2*(y_pred-X1)*derivative_sigmoid(out_3)\n",
    "        tot,bia = backprop(delta,z)\n",
    "        total_3 = total_3 + tot\n",
    "        bias_3 = bias_3 + bia\n",
    "    \n",
    "    ## encoder's mean part\n",
    "        sm = smi(delta,W_decoder,out_1)\n",
    "        sm_mean = sm + Lambda*zmean*derivative_sigmoid(out_1)\n",
    "        tot,bia = backprop(sm_mean,X1)\n",
    "        total_1 = total_1 + tot\n",
    "        bias_1 = bias_1 +bia\n",
    "    \n",
    "    ## encoder's covar part \n",
    "        sm1 = smi(delta,W_decoder,out_2)\n",
    "        sm_covar = 0.5*sm1*a*np.exp(0.5*zlogvar) + Lambda*0.5*(np.exp(zlogvar)-1)*derivative_sigmoid(out_2)\n",
    "        tot,bia = backprop(sm_covar,X1)\n",
    "        total_2 = total_2 + tot\n",
    "        bias_2 = bias_2 + bia\n",
    "    ## backprop:\n",
    "        b_decoder = b_decoder - learning_rate*bias_3\n",
    "        b_mean = b_mean - learning_rate*bias_1\n",
    "        b_covar = b_covar - learning_rate*bias_2\n",
    "        W_decoder = W_decoder - learning_rate*total_3\n",
    "        W_mean = W_mean - learning_rate*total_1\n",
    "        W_covar = W_covar - learning_rate*total_2\n",
    "    print(\"epoch: \"+str(i) +\" \"+\"loss: \"+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "z1 = np.random.normal(0,1,(1000,40))\n",
    "out = layer(W_decoder,z1,b_decoder)\n",
    "y1 = sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADhVJREFUeJzt3VuMXeV5xvHnmfGBGTtkBle2HA8UCwG1BW4xViCkSsGmYkwQ9kUvQKWCJpIl1DYkihQZcRH1rlKiKJEaJbIwiWksuHBIQUhJcUmiqFKDAsaiPtslsRl8jCx8xOPDvL3Y25Izre3x+tZeM9P3/5NGs/ee9c77zWieWWuvvb/1OSIEIJ+u8R4AgPFB+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJDWlyWa2w3aTLYFUIkIRMaaQNR1+TZ8+vcmWQCrDw8Nj3pbDfiApwg8kVRR+24O2d9reY3t1XYMC0HmVw2+7W9J3JS2XtFDS47YX1jUwAJ1Vsuf/tKQ9EfF+RJyV9LKkFfUMC0CnlYR/nqQPLrk/1H4MwCTQ8Zf6bK+StKrTfQBcm5LwfyjpxkvuD7Qf+wMRsUbSGknq6urimmHABFFy2P8bSbfanm97mqTHJL1Wz7AAdFrlPX9EnLf995L+TVK3pBciYmttIwPQUW7y6r1dXV3B23uBzhkeHtbIyMiY3tvPO/yApAg/kBThB5JqdErvZFZyHYLu7u6i3iXnSXp7e4t6z5gxo6h+ZGSkcu3JkyeLepfUnzt3rqj3ZFgJiz0/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gqTRTeru6yv7P9fT0VK4dGBgo6l0yrba/v7+o90033VRUX2L//v1F9adOnapcu3PnzqLeR48erVx74cKFot5jxZ4fSIrwA0kRfiApwg8kVbJE9422f2F7m+2ttp+pc2AAOqvkbP95SV+NiE22PyHpHdsbI2JbTWMD0EGV9/wRcSAiNrVvn5C0XSzRDUwatbzOb/tmSXdJeuv/+BpLdAMTUHH4bc+U9GNJX46I46O/zhLdwMRUdLbf9lS1gr8+Il6pZ0gAmlBytt+S1kraHhHfqm9IAJpQsuf/rKS/kbTU9ub2x8M1jQtAh1V+zh8R/yGp+gJ2AMYV7/ADkiL8QFJp5vOXLHMtlc3JL+390EMPVa5duXJlUe++vr6i+pLrKAwNDRX1XrduXeXa0iW2N2/eXLm25DoE14I9P5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+IKlJNaW3ZHpob29vUe+SJbrvvPPOot73339/5drSabFr164tqi9ZIrx0OvLdd99duXbv3r1FvVuXuJzY2PMDSRF+ICnCDyRF+IGkisNvu9v2u7Zfr2NAAJpRx57/GbVW6AUwiZSu1Tcg6fOSnq9nOACaUrrn/7akr0kaudwGtlfZftv226WXQwZQn5KFOh+RdDgi3rnSdhGxJiKWRMSSyfDGByCL0oU6H7X9O0kvq7Vg549qGRWAjqsc/oh4NiIGIuJmSY9J+nlEPFHbyAB0FK/zA0nVMrEnIn4p6Zd1fC8AzWDPDyRF+IGkJtV8/hJTp04tqp81a1bl2kWLFhX1/vjjjyvXvvjii0W9N2zYUFS/dOnSyrWDg4NFvUuXRi9x4cKFces9Vuz5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSU2qKb0ll/6eMqXsR509e3bl2tIluvft21e5tnSp6QULFhTVP/3005VrFy9eXNR7165dlWtHRi57Nfox6e7uLqpvAnt+ICnCDyRF+IGkCD+QVOlCnX22N9jeYXu77c/UNTAAnVV6tv87kn4WEX9le5qk3hrGBKABlcNv+5OSPifpKUmKiLOSztYzLACdVnLYP1/SEUk/sP2u7edtzxi9EUt0AxNTSfinSFos6XsRcZekU5JWj96IJbqBiakk/EOShiLirfb9DWr9MwAwCZQs0X1Q0ge2b28/tEzStlpGBaDjSs/2/4Ok9e0z/e9L+tvyIQFoQlH4I2KzpCU1jQVAg3iHH5AU4QeSmlTz+UvMnDmzqH7+/PmVa2+55Zai3keOHKlcu3z58qLeJT+3JD3wwAOVa6dNm1bUu2SZ7K6usv0i8/kBTFiEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kNSkms9fcunvvr6+ot79/f2Va0vndl933XWVa6+//vqi3seOHSuq37FjR+Xa2267raj38PBw5drSNSZGRkaK6pvAnh9IivADSRF+IKnSJbq/Ynur7S22X7Jd/ckpgEZVDr/teZK+JGlJRNwhqVvSY3UNDEBnlR72T5HUY3uKpF5J+8uHBKAJJWv1fSjpm5L2STog6VhEvDF6O5boBiamksP+fkkrJM2X9ClJM2w/MXo7lugGJqaSw/4HJf02Io5ExDlJr0i6r55hAei0kvDvk3Sv7V63dunLJG2vZ1gAOq3kOf9bkjZI2iTpv9rfa01N4wLQYaVLdH9d0tdrGguABvEOPyApwg8kNamm9JYsm9zT01PU+9SpU5Vr9+7dW9T79OnTlWtPnDhR1Hv27NlF9QsWLKhcWzoVemhoqHLtmTNninqfP3++qL4J7PmBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gqUk1n79E6fzsAwcOVK6dPn16Ue/BwcHKtffdV3ZB5ZJlriXpo48+qlz76quvFvXetGlT5dqDBw8W9T537lxRfRPY8wNJEX4gKcIPJHXV8Nt+wfZh21sueewG2xtt725/7u/sMAHUbSx7/h9KGn3GabWkNyPiVklvtu8DmESuGv6I+JWko6MeXiFpXfv2Okkrax4XgA6r+lLfnIi4+NrXQUlzLreh7VWSVlXsA6BDil/nj4iwHVf4+hq11/Dr6uq67HYAmlX1bP8h23Mlqf35cH1DAtCEquF/TdKT7dtPSip7KxaAxo3lpb6XJP2npNttD9n+oqR/kvSXtndLerB9H8AkctXn/BHx+GW+tKzmsQBoEO/wA5Ii/EBSk2pK74ULFyrX7t69u6j3yMhI5dp9+/YV9b7nnnsq15ZOyd26dWtRfenPXuLkyZOVaw8dOlTUu+RvtSns+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApRzR3Ne2urq4oXa66oHdRfU9PT+XaOXMuu6zBmJQsF3306Oj1Vq7NrFmziurnzp1bufbYsWNFvY8fP165tnSJ7ZLrP5QYHh7WyMiIx7Ite34gKcIPJEX4gaSqLtH9Dds7bL9n+ye2+zo7TAB1q7pE90ZJd0TEIkm7JD1b87gAdFilJboj4o2ION+++2tJAx0YG4AOquM5/xck/bSG7wOgQUXX7bf9nKTzktZfYZtVklaV9AFQv8rht/2UpEckLYsrvFMoItZIWiO13uRTtR+AelUKv+1BSV+T9BcRcbreIQFoQtUluv9Z0ickbbS92fb3OzxOADWrukT32g6MBUCDeIcfkBThB5JKM6W3lD2mWZK110pl05FLe5cq+fsqnRZb0rvJXNSJKb0ArorwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiq6dPe1iojfnzlzZu8VNvkjSb9vajz0pvf/w95/PNYNG72Yx9XYfjsiltCb3vTuPA77gaQIP5DURAv/GnrTm97NmFDP+QE0Z6Lt+QE0ZEKE3/ag7Z2299he3WDfG23/wvY221ttP9NU70vG0G37XduvN9y3z/YG2ztsb7f9mQZ7f6X9+95i+yXb13W43wu2D9vecsljN9jeaHt3+3N/g72/0f69v2f7J7b7OtH7asY9/La7JX1X0nJJCyU9bnthQ+3PS/pqRCyUdK+kv2uw90XPSNrecE9J+o6kn0XEn0j606bGYHuepC9JWhIRd0jqlvRYh9v+UNLgqMdWS3ozIm6V9Gb7flO9N0q6IyIWSdol6dkO9b6icQ+/pE9L2hMR70fEWUkvS1rRROOIOBARm9q3T6gVgHlN9JYk2wOSPi/p+aZ6tvt+UtLn1F5zMSLORsRHDQ5hiqQe21Mk9Ura38lmEfErSUdHPbxC0rr27XWSVjbVOyLeiIjz7bu/ljTQid5XMxHCP0/SB5fcH1KDAbzI9s2S7pL0VoNtv63WUudlS9Ncu/mSjkj6Qfspx/O2ZzTROCI+lPRNSfskHZB0LCLeaKL3KHMi4kD79kFJc8ZhDJL0BUk/HY/GEyH84872TEk/lvTliDjeUM9HJB2OiHea6DfKFEmLJX0vIu6SdEqdO+z9A+3n1ivU+gf0KUkzbD/RRO/LidZLXo2/7GX7ObWeeq5vurc0McL/oaQbL7k/0H6sEbanqhX89RHxSlN9JX1W0qO2f6fWU52ltn/UUO8hSUMRcfEoZ4Na/wya8KCk30bEkYg4J+kVSfc11PtSh2zPlaT258NNNrf9lKRHJP11jNPr7RMh/L+RdKvt+banqXXy57UmGru1iuVaSdsj4ltN9LwoIp6NiIGIuFmtn/nnEdHIHjAiDkr6wPbt7YeWSdrWRG+1Dvfvtd3b/v0v0/ic8HxN0pPt209KerWpxrYH1Xq692hEnG6q7/8SEeP+Ielhtc56/rek5xrs++dqHe69J2lz++Phcfj575f0esM9/0zS2+2f/V8l9TfY+x8l7ZC0RdK/SJre4X4vqXV+4ZxaRz1flDRLrbP8uyX9u6QbGuy9R63zXBf/5r7f9N9cRPAOPyCriXDYD2AcEH4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSOp/AHF7EkBbPxbGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y2 = y1[-1]*255\n",
    "y2 = y2.reshape(14,14)\n",
    "plt.imshow(y2,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
